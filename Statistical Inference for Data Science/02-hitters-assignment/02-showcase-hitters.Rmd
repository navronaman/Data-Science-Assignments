---
title: 'Assignment 2: Hitters data in ISLR'
author: "Naman Shah"
date: "`r Sys.Date()`"
output: word_document
---

Loading the libraries, creating a color vector and setting a seed.

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, results='hide', include=FALSE, echo=TRUE}
library("ISLR")
library("tidyverse")
library('moderndive')
library('skimr')
library('lars')
```

```{r}

colors_vector <- c("#1f78b4", "#33a02c", "#e31a1c", "#ff7f00", "#6a3d9a",
                   "#b15928", "#a6cee3", "#b2df8a", "#fb9a99", "#fdbf6f",
                   "#cab2d6", "#ffff99", "#b15928", "#8dd3c7", "#bebada")


set.seed(123)
```

Establishing subsets for each division for model building purposes.

```{r}
hitters <- Hitters
noNAHitter <- na.omit(hitters)
hittersN <- noNAHitter[noNAHitter$League == 'N',]
hittersA <- noNAHitter[noNAHitter$League == 'A',]
hittersNE <- hittersN[hittersN$Division == 'E',]
hittersNW <- hittersN[hittersN$Division == 'W',]
hittersAE <- hittersA[hittersA$Division == 'E',]
hittersAW <- hittersA[hittersA$Division == 'W',]

boxplot(hittersNE$Salary, hittersNW$Salary, 
        hittersAE$Salary, hittersAW$Salary,
        col=colors_vector, main="Boxplot of salaries of different divisions")

mainBoxLegend = c('NE Salary', 'NW Salary', 'AE Salary', 'AW Salary')
legend('topright', legend = mainBoxLegend, fill = colors_vector, border = 'black', cex = 0.7)

```

The median of each division is effectively the same, though with the AE division having a slightly lower one. The interquartile range for all are also roughly the same, with the AW division having the highest upper quartile and NE having the lowest.

#Building Lars models for all divisions
```{r}
#AE
XAE <- hittersAE[, -c(20, 19, 14, 15)]
YAE <- hittersAE$Salary
larsAE <- lars(as.matrix(XAE), YAE)
larsAE
min(larsAE$Cp)
larsAE$Cp == min(larsAE$Cp)
#18, so adjusted index would be 19
larsAE$beta[19,]
larsAE$Cp[19]

#AW
XAW <- hittersAW[, -c(20, 19, 14, 15)]
YAW <- hittersAW$Salary
larsAW <- lars(as.matrix(XAW), YAW)
larsAW
min(larsAW$Cp)
larsAW$Cp == min(larsAW$Cp)
#4, so adjusted index would be 5
larsAW$beta[5,]
larsAW$Cp[5]

#NE
XNE <- hittersNE[, -c(20, 19, 14, 15)]
YNE <- hittersNE$Salary
larsNE <- lars(as.matrix(XNE), YNE)
larsNE
min(larsNE$Cp)
larsNE$Cp == min(larsNE$Cp)
#8, so adjusted index would be 9
larsNE$beta[9,]
larsNE$Cp[9]

#NW
XNW <- hittersNW[, -c(20, 19, 14, 15)] 
YNW <- hittersNW$Salary
larsNW <- lars(as.matrix(XNW), YNW)
larsNW 
min(larsNW$Cp)
larsNW$Cp == min(larsNW$Cp)
#16, so adjusted index would be 17
larsNW$beta[17,]
larsNW$Cp[17]



```

For the NE and NW models, it seems to the the walks, putouts, CRBI, Assists, HmRuns and years which play major roles.

#Building LM models for each division
For reference, a scatterplot with red indicates LARs and a scatterplot with blue indicated LM.
```{r}
#AE 
plot(as.matrix(XAE)%*%larsAE$beta[19,] + larsAE$mu, YAE, main = "AE with AE, lars", col="red")
cor(as.matrix(XAE)%*%larsAE$beta[19,] + larsAE$mu, YAE)
cor(as.matrix(XAE)%*%larsAE$beta[19,] + larsAE$mu, YAE)^2
#0.889, r-squared = 0.790
lmAEBase <- cbind(XAE, YAE)
lmAE <- lm(YAE~., data = lmAEBase)
plot(predict(lmAE, XAE), YAE, main = 'AE with AE, lm', col="blue")

#AW
plot(as.matrix(XAW)%*%larsAW$beta[19,] + larsAW$mu, YAW, main = "AW with AW, lars", col="red")
cor(as.matrix(XAW)%*%larsAW$beta[19,] + larsAW$mu, YAW)
cor(as.matrix(XAW)%*%larsAW$beta[19,] + larsAW$mu, YAW)^2
#0.853, r-squared = 0.727
lmAWBase <- cbind(XAW, YAW)
lmAW <- lm(YAW~., data = lmAWBase)
plot(predict(lmAW, XAW), YAW, main = 'AW with AW, lm', col="blue")

#NE
plot(as.matrix(XNE)%*%larsNE$beta[19,] + larsNE$mu, YNE, main = "NE with NE, lars", col="red")
cor(as.matrix(XNE)%*%larsNE$beta[19,] + larsNE$mu, YNE)
cor(as.matrix(XNE)%*%larsNE$beta[19,] + larsNE$mu, YNE)^2
#0.725, r-squared = 0.525
lmNEBase <- cbind(XNE, YNE)
lmNE <- lm(YNE~., data = lmNEBase)
plot(predict(lmNE, XNE), YNE, main="NE with NE, lm", col="blue")

#NW
plot(as.matrix(XNW)%*%larsNW$beta[19,] + larsNW$mu, YNW, main = 'NW with NW, lars', col="red")
cor(as.matrix(XNW)%*%larsNW$beta[19,] + larsNW$mu, YNW)
cor(as.matrix(XNW)%*%larsNW$beta[19,] + larsNW$mu, YNW)^2
#0.775, r-squared = 0.600
lmNWBase <- cbind(XNW, YNW)
lmNW <- lm(YNW~., data = lmNWBase)
plot(predict(lmNW, XNW), YNW, main="NW with NW, lm", col="blue")

```
# Plotting out NE lm model results
```{r}
plot(predict(lmAE, XNE), YNE, main = 'NE with lmAE', col="blue")
lmNEwAE <- predict(lmAE, XNE)

plot(predict(lmAW, XNE), YNE, main = "NE with lmAW", col="blue")
lmNEwAW <- predict(lmAW, XNE)

plot(predict(lmNE, XNE), YNE, main = "NE with lmNE", col="blue")
lmNEwNE <- predict(lmNE)

plot(predict(lmNW, XNE), YNE, main = "NE with lmNW", col="blue")
lmNEwNW <- predict(lmNW, XNE)
```
The best bet we have here is of course NE's own LM model with itself, but that is also far from perfect, as there isn't the best linear relationship.
NE's predictions on other predictions are not any better, as they are predicting non-fuctional relationships.

# Predicting out NE lm model results

```{r}
plot(predict(lmAE, XNW), YNW, main = 'NW with lmAE', col="blue")
lmNWwAE <- predict(lmAE, XNW)

plot(predict(lmAW, XNW), YNW, main = "NW with lmAW", col="blue")
lmNWwAW <- predict(lmAW, XNW)

plot(predict(lmNE, XNW), YNW, main = "NW with lmNE", col="blue")
lmNWwNE <- predict(lmNE, XNW)

plot(predict(lmNW, XNW), YNW, main = "NW with lmNW", col="blue")
lmNWwNW <- predict(lmNW)

```

This time, the LM model for NW does make a neat linear graph for the actual NW, excluding a few outliers. As expected, it does a bad job at predicting NW and AW salaries, and doing a half baked job with AE.
#Establishing lars models for NE and NW and adjusting for means
```{r}
#NE
NE.AE <- as.matrix(XNE)%*%larsAE$beta[19,]
NE.AW <- as.matrix(XNE)%*%larsAW$beta[5,]
NE.NE <- as.matrix(XNE)%*%larsNE$beta[9,]
NE.NW <- as.matrix(XNE)%*%larsNW$beta[17,]

#NW
NW.AE <- as.matrix(XNW)%*%larsAE$beta[19,]
NW.AW <- as.matrix(XNW)%*%larsAW$beta[5,]
NW.NE <- as.matrix(XNW)%*%larsNE$beta[9,]
NW.NW <- as.matrix(XNW)%*%larsNW$beta[17,]

```
#Plots with for NE lars models
```{r}
plot(as.matrix(NE.AE), hittersNE$Salary, main = 'NE with AE, lars',
     xlab = 'lars', ylab = 'Actual', col="red")
plot(as.matrix(NE.AW), hittersNE$Salary, main = 'NE with AW, lars',
     xlab = 'lars', ylab = 'Actual', col="red")
plot(as.matrix(NE.NE), hittersNE$Salary, main = 'NE with NE, lars',
     xlab = 'lars', ylab = 'Actual', col="red")
plot(as.matrix(NE.NW), hittersNE$Salary, main = 'NE with NW, lars',
     xlab = 'lars', ylab = 'Actual', col="red")

```

Compared to the LM model, the lars model does the same or slightly worse job predicting the NE salaries. This tells us the NE model generally is slightly tougher to predict compared to the NW model.
The lars model of NE does a really bad job at predicting AE, AW and NW, which the LM model was slightly better at.
#Plots for NW lars model
```{r}
plot(as.matrix(NW.AE), hittersNW$Salary, main = 'NW with AE, lars',
     xlab = 'lars', ylab = 'Actual', col="red")
plot(as.matrix(NW.AW), hittersNW$Salary, main = 'NW with AW, lars',
     xlab = 'lars', ylab = 'Actual', col="red")
plot(as.matrix(NW.NE), hittersNW$Salary, main = 'NW with NE, lars',
     xlab = 'lars', ylab = 'Actual', col="red")
plot(as.matrix(NW.NW), hittersNW$Salary, main = 'NW with NW, lars',
     xlab = 'lars', ylab = 'Actual', col="red")

```
Much like the LM model, the NW salaries are predicted fairly impresively by the Lars model, and once again the model does bad jobs at predicting other salaries, espicially the AE model.

Upon examining the actual coefficients for the two models, it can be shown that the lm and lars models have high coefficients for comparable events, specifically hits, walks, and cruns.
But there are other factors that the lars model just ignores, including years (18), which LM takes into serious consideration.

Additionally, certain variables in the lars model have no association at all, but other variables in the lm have a negative correlation. For example, the lars does not take mistakes into account at all, but the lm views them as rather negatve when estimating salary.

#Boxplots (extra credit)
#NE and NW lm boxplots
```{r}

box_c = c("#1f78b4", "#33a02c")

boxplot(hittersNE$Salary, lmNEwNE, 
        hittersAE$Salary, lmNEwAE, 
        hittersAW$Salary, lmNEwAW, 
        hittersAW$Salary, lmNEwNW, 
        main = 'NE based models compared with all, lm', col = box_c)
lmAWLegend = c('Actual Salary (NE)', 
               'NE with NE',
               'Actual Salary (AE)', 
               'NE with AE',
               'Actual Salary (AW)',
               'NE with AW',
               'Actual Salary (NW)',
               'NE with NW')
legend('topright', legend = lmAWLegend, fill = box_c, border = 'black', cex = 0.5)

boxplot(hittersNW$Salary, lmNWwNW, 
        hittersAE$Salary, lmNWwAE, 
        hittersAW$Salary, lmNWwAW, 
        hittersNE$Salary, lmNWwNE, 
        main = 'NW based models compared with all, lm', col = box_c)
lmAWLegend = c('Actual Salary (NW)', 
               'NW with NW',
               'Actual Salary (AE)', 
               'NW with AE', 
               'Actual Salary (AW)',
               'NW with AW',
               'Actual Salary (NW)',
               'NE with NW')
legend('topright', legend = lmAWLegend, fill = box_c, border = 'black', cex = 0.5)


```
We see the boxplots of the LM models (green) get the medians and middle 50% range of the actual salaries (blue) fairly accurately. All other predictions using the NE's lm models fall off from the actual salaries by a lot, especially considering

The same can be said for NW, the LM model for NW is good at understanding the main matter for NW, but it doesn't do a good job at predicting other models, even failing at getting their medians.

#NE and NW lars boxplots
Normalizing the means is the first step
```{r}
NE.AE <- NE.AE - mean(NE.AE) + larsAE$mu
NE.AW <- NE.AW - mean(NE.AW) + larsAW$mu
NE.NE <- NE.NE - mean(NE.NE) + larsNE$mu
NE.NW <- NE.NW - mean(NE.NW) + larsNW$mu

NW.AE <- NW.AE - mean(NW.AE) + larsAE$mu
NW.AW <- NW.AW - mean(NW.AW) + larsAW$mu
NW.NE <- NW.NE - mean(NW.NE) + larsNE$mu
NW.NW <- NW.NW - mean(NW.NW) + larsNW$mu 

boxplot(hittersNE$Salary, c(as.matrix(NE.NE)), 
        hittersAE$Salary, c(as.matrix(NE.AE)),
        hittersAW$Salary, c(as.matrix(NE.AW)),
        hittersNW$Salary, c(as.matrix(NE.NW)),
        main = 'NE models compared to All, lars', col = box_c)
larsNELegend = c('Actual Salary (NE)', 'NE with NE',
                 'Actual Salary (AE)','NE with AE', 
                 'Actual Salary (AW)','NE with AW',
                 'Actual Salary (NW)','NE with NW')
legend('topright', legend = larsNELegend, fill = box_c, border = 'black', cex = 0.5)


boxplot(hittersNW$Salary, c(as.matrix(NW.NW)), 
        hittersAE$Salary, c(as.matrix(NW.AE)),
        hittersAW$Salary, c(as.matrix(NW.AW)),
        hittersNW$Salary, c(as.matrix(NW.NW)),
        main = 'NW models compared to All, lars', col = box_c)
larsNELegend = c('Actual Salary (NW)', 'NW with NW',
                 'Actual Salary (AE)','NW with AE', 
                 'Actual Salary (AW)','NW with AW',
                 'Actual Salary (NE)','NW with NW')
legend('topright', legend = larsNELegend, fill = box_c, border = 'black', cex = 0.5)



```

The lars model do a better job at getting the ranges right, but their medians are off, and their upper ranges are definitely not in line with anything we're trying to predict. This is true for example, predicting the AE salaries using AE and NE lars model, which does a really bad job at predicting it.

